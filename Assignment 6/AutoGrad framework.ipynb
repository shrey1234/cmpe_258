{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoGrad framework.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Dlw5klusXSt6"},"source":["from torchvision.datasets import MNIST\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-soWM6_GkTw"},"source":["class Tensor(object):\n","\n","  def __init__(self, data, requires_grad=False):\n","      self.data = data\n","      if not isinstance(data, np.ndarray):\n","          self.data = np.array(data)\n","      self.requires_grad = requires_grad\n","      self._grad = None\n","      self._grad_fn = None\n","\n","  def shape(self):\n","      return self.data.shape\n","\n","  def grad_fn(self):\n","      return self._grad_fn\n","\n","  def grad(self):\n","      return self._grad\n","\n","  def backward(self, grad=None):\n","\n","      if grad is None and self._grad is None:\n","          grad = self.__class__(1., requires_grad=False)\n","\n","      elif self.grad is not None:\n","          grad = self._grad\n","\n","      self.grad_fn.backward(grad)\n","      return True\n","\n","  def __repr__(self):\n","      return str(self.data.__repr__())\n","\n","  def add_grad(self, grad):\n","      if self._grad is None:\n","          self._grad = grad\n","      else:\n","          self._grad += grad\n","\n","\n","class AddOp(object):\n","  def forward(self, x: Tensor, y: Tensor):\n","      self.x = x\n","      self.y = y\n","      requires_grad = x.requires_grad or y.requires_grad\n","      return Tensor(x.data + y.data, requires_grad=requires_grad)\n","\n","  def backward(self, grad):\n","      if self.x.requires_grad:\n","          self.x.add_grad(Tensor(grad.data.sum(axis=axis, keepdims=True)))\n","          if self.x.grad_fn:\n","              self.x.backward()\n","      if self.y.requires_grad:\n","          self.y.add_grad(Tensor(grad.data.sum(axis=axis, keepdims=True)))\n","          if self.y.grad_fn:\n","              self.y.backward()\n","\n","class Multiplication(object):\n","  def forward(self, x: Tensor, y: Tensor):\n","      self.x = x\n","      self.y = y\n","      requires_grad = x.requires_grad or y.requires_grad\n","      return Tensor(x.data * y.data, requires_grad=requires_grad)\n","\n","  def backward(self, grad):\n","      if self.x.requires_grad:\n","          self.x.add_grad(Tensor(grad.data * self.y.data, False))\n","          if self.x.grad_fn:\n","              self.x.backward()\n","      if self.y.requires_grad:\n","          self.y.add_grad(Tensor(grad.data * self.x.data, False))\n","          if self.y.grad_fn:\n","              self.y.backward()\n","\n","class Negation(object):\n","  def forward(self, x: Tensor):\n","      self.x = x\n","      requires_grad = x.requires_grad \n","      return Tensor(x.data * (-1), requires_grad=requires_grad)\n","\n","  def backward(self, grad):\n","      if self.x.requires_grad:\n","          self.x.add_grad(Tensor(grad.data * -1, False))\n","          if self.x.grad_fn:\n","              self.x.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNis1sIgUfI7"},"source":["class Layer:\n","\n","  def __call__(self, *args):\n","      return self.forward(*args)\n","\n","class Relu(Layer):\n","\n","  def forward(self,x):\n","      self.x = x\n","      return np.maximum(np.zeros_like(x), x)\n","    \n","  def backward(self, grad):\n","      grad_input = (self.x > 0) * grad\n","      return grad_input\n","\n","class MSE(Layer):\n","\n","  def forward(self, x, y):\n","      self.x = x\n","      self.y = y\n","      return ((x - y)**2) / (self.x.shape[0]*2)\n","\n","  def backward(self, grad=None):\n","      return (self.x - self.y) / self.x.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYZBLP61Umjg"},"source":["class Linear(Layer):\n","\n","  def __init__(self, input, output, lr=0.0002):\n","    super().__init__()\n","    self.weight = 2*np.random.random((input, output)) - 1\n","    self.bias = 2*np.random.random((output)) - 1\n","    self.lr = lr\n","\n","  def forward(self, x):\n","    self.x = x\n","    return np.dot(x,self.weight) + self.bias\n","\n","  def backward(self, grad):\n","    bias_grad = grad.mean(axis=0)*self.x.shape[0]\n","    weight_grad = np.dot(self.x.T, grad)\n","    grad_input = np.dot(grad, self.weight.T)\n","    \n","    self.weight -= weight_grad * self.lr\n","    self.bias -= bias_grad * self.lr\n","\n","    return grad_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRZckLubUq1f"},"source":["\n","\n","class Model(Layer):\n","  def __init__(self, lr=0.00001):\n","      self.lr = lr\n","      self.layers = [\n","          Linear(784,400, lr=self.lr),\n","          Relu(),\n","          Linear(400,100, lr=self.lr),\n","          Relu(),\n","          Linear(100,10, lr=self.lr)        \n","      ]\n","\n","  def forward(self,x):\n","      for l in self.layers:\n","          x = l(x)\n","      return x\n","\n","  def backward(self, grad):\n","      for l in self.layers[::-1]:\n","          grad = l.backward(grad)\n","\n","      return grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FiLq_W3dXA45"},"source":["simple = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","ds = MNIST('./mnist', download=True, transform=simple)\n","ld = DataLoader(ds, batch_size=2, pin_memory=True, drop_last=True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMsAv5FXUwvq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619820787987,"user_tz":420,"elapsed":667781,"user":{"displayName":"Shreya Goyal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVsl18G8XwoIr9Ou5nfrZdpmwxA-g-n7XxqlSX=s64","userId":"05010597830317148669"}},"outputId":"8c617e5b-fa7c-4e20-8cd8-a102c856fca0"},"source":["mm = Model()\n","loss = MSE()\n","_loss_avg = 0 \n","for epochs in range(10):\n","    cnt = 0\n","    total = 0\n","    for i, (img, label) in enumerate(ld):\n","        x = img.view(2,-1).numpy()\n","        res = mm(x)\n","        _loss = loss(res, label.numpy())\n","        total += label.data.size(0)\n","        cnt += int(np.argmax(res) == np.argmax(label.numpy()))\n","        _loss_avg += _loss.mean() \n","        grad = loss.backward(1)\n","        mm.backward(grad)\n","        if i % 1000 == 0:\n","          print('Train - Epoch %d, Batch: %d, Loss: %f' % (epochs, i, _loss_avg/1000))\n","          _loss_avg = 0\n","        loss.backward()\n","        mm.backward(grad)\n","    print(' Accuracy: %f' % (float(cnt) / len(ds)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train - Epoch 0, Batch: 0, Loss: 0.323625\n","Train - Epoch 0, Batch: 1000, Loss: 124.349192\n","Train - Epoch 0, Batch: 2000, Loss: 66.812781\n","Train - Epoch 0, Batch: 3000, Loss: 50.472935\n","Train - Epoch 0, Batch: 4000, Loss: 43.582088\n","Train - Epoch 0, Batch: 5000, Loss: 36.746565\n","Train - Epoch 0, Batch: 6000, Loss: 30.987646\n","Train - Epoch 0, Batch: 7000, Loss: 32.905916\n","Train - Epoch 0, Batch: 8000, Loss: 28.759991\n","Train - Epoch 0, Batch: 9000, Loss: 27.538699\n","Train - Epoch 0, Batch: 10000, Loss: 22.896226\n","Train - Epoch 0, Batch: 11000, Loss: 20.877211\n","Train - Epoch 0, Batch: 12000, Loss: 21.706594\n","Train - Epoch 0, Batch: 13000, Loss: 20.512052\n","Train - Epoch 0, Batch: 14000, Loss: 20.801178\n","Train - Epoch 0, Batch: 15000, Loss: 19.861267\n","Train - Epoch 0, Batch: 16000, Loss: 20.947768\n","Train - Epoch 0, Batch: 17000, Loss: 17.711630\n","Train - Epoch 0, Batch: 18000, Loss: 17.718749\n","Train - Epoch 0, Batch: 19000, Loss: 16.721489\n","Train - Epoch 0, Batch: 20000, Loss: 16.093615\n","Train - Epoch 0, Batch: 21000, Loss: 16.156702\n","Train - Epoch 0, Batch: 22000, Loss: 14.709905\n","Train - Epoch 0, Batch: 23000, Loss: 16.598154\n","Train - Epoch 0, Batch: 24000, Loss: 15.000131\n","Train - Epoch 0, Batch: 25000, Loss: 14.819498\n","Train - Epoch 0, Batch: 26000, Loss: 14.331626\n","Train - Epoch 0, Batch: 27000, Loss: 13.398671\n","Train - Epoch 0, Batch: 28000, Loss: 13.080350\n","Train - Epoch 0, Batch: 29000, Loss: 11.275718\n"," Accuracy: 0.018300\n","Train - Epoch 1, Batch: 0, Loss: 9.241702\n","Train - Epoch 1, Batch: 1000, Loss: 12.305036\n","Train - Epoch 1, Batch: 2000, Loss: 11.083548\n","Train - Epoch 1, Batch: 3000, Loss: 11.235021\n","Train - Epoch 1, Batch: 4000, Loss: 11.348221\n","Train - Epoch 1, Batch: 5000, Loss: 11.900752\n","Train - Epoch 1, Batch: 6000, Loss: 10.894979\n","Train - Epoch 1, Batch: 7000, Loss: 12.349787\n","Train - Epoch 1, Batch: 8000, Loss: 11.783463\n","Train - Epoch 1, Batch: 9000, Loss: 11.649317\n","Train - Epoch 1, Batch: 10000, Loss: 9.851736\n","Train - Epoch 1, Batch: 11000, Loss: 9.368113\n","Train - Epoch 1, Batch: 12000, Loss: 10.116241\n","Train - Epoch 1, Batch: 13000, Loss: 10.605702\n","Train - Epoch 1, Batch: 14000, Loss: 10.723605\n","Train - Epoch 1, Batch: 15000, Loss: 11.452698\n","Train - Epoch 1, Batch: 16000, Loss: 11.771515\n","Train - Epoch 1, Batch: 17000, Loss: 9.653962\n","Train - Epoch 1, Batch: 18000, Loss: 9.755472\n","Train - Epoch 1, Batch: 19000, Loss: 9.970889\n","Train - Epoch 1, Batch: 20000, Loss: 9.913112\n","Train - Epoch 1, Batch: 21000, Loss: 9.727042\n","Train - Epoch 1, Batch: 22000, Loss: 8.990656\n","Train - Epoch 1, Batch: 23000, Loss: 10.278366\n","Train - Epoch 1, Batch: 24000, Loss: 9.914845\n","Train - Epoch 1, Batch: 25000, Loss: 9.965137\n","Train - Epoch 1, Batch: 26000, Loss: 9.541338\n","Train - Epoch 1, Batch: 27000, Loss: 9.034261\n","Train - Epoch 1, Batch: 28000, Loss: 8.867218\n","Train - Epoch 1, Batch: 29000, Loss: 7.426804\n"," Accuracy: 0.017150\n","Train - Epoch 2, Batch: 0, Loss: 6.119220\n","Train - Epoch 2, Batch: 1000, Loss: 8.662569\n","Train - Epoch 2, Batch: 2000, Loss: 7.418907\n","Train - Epoch 2, Batch: 3000, Loss: 7.778163\n","Train - Epoch 2, Batch: 4000, Loss: 7.718032\n","Train - Epoch 2, Batch: 5000, Loss: 8.433404\n","Train - Epoch 2, Batch: 6000, Loss: 7.987546\n","Train - Epoch 2, Batch: 7000, Loss: 8.700096\n","Train - Epoch 2, Batch: 8000, Loss: 8.563092\n","Train - Epoch 2, Batch: 9000, Loss: 8.331007\n","Train - Epoch 2, Batch: 10000, Loss: 7.119155\n","Train - Epoch 2, Batch: 11000, Loss: 6.932872\n","Train - Epoch 2, Batch: 12000, Loss: 7.376910\n","Train - Epoch 2, Batch: 13000, Loss: 7.964577\n","Train - Epoch 2, Batch: 14000, Loss: 7.942566\n","Train - Epoch 2, Batch: 15000, Loss: 8.860488\n","Train - Epoch 2, Batch: 16000, Loss: 8.934848\n","Train - Epoch 2, Batch: 17000, Loss: 7.230298\n","Train - Epoch 2, Batch: 18000, Loss: 7.174779\n","Train - Epoch 2, Batch: 19000, Loss: 7.686865\n","Train - Epoch 2, Batch: 20000, Loss: 7.579951\n","Train - Epoch 2, Batch: 21000, Loss: 7.494568\n","Train - Epoch 2, Batch: 22000, Loss: 6.942207\n","Train - Epoch 2, Batch: 23000, Loss: 7.825599\n","Train - Epoch 2, Batch: 24000, Loss: 7.967274\n","Train - Epoch 2, Batch: 25000, Loss: 7.957273\n","Train - Epoch 2, Batch: 26000, Loss: 7.649689\n","Train - Epoch 2, Batch: 27000, Loss: 7.142171\n","Train - Epoch 2, Batch: 28000, Loss: 7.126181\n","Train - Epoch 2, Batch: 29000, Loss: 5.855670\n"," Accuracy: 0.017517\n","Train - Epoch 3, Batch: 0, Loss: 4.829249\n","Train - Epoch 3, Batch: 1000, Loss: 6.956088\n","Train - Epoch 3, Batch: 2000, Loss: 5.871998\n","Train - Epoch 3, Batch: 3000, Loss: 6.241262\n","Train - Epoch 3, Batch: 4000, Loss: 6.166207\n","Train - Epoch 3, Batch: 5000, Loss: 6.885120\n","Train - Epoch 3, Batch: 6000, Loss: 6.464110\n","Train - Epoch 3, Batch: 7000, Loss: 6.964953\n","Train - Epoch 3, Batch: 8000, Loss: 6.983434\n","Train - Epoch 3, Batch: 9000, Loss: 6.786631\n","Train - Epoch 3, Batch: 10000, Loss: 5.843219\n","Train - Epoch 3, Batch: 11000, Loss: 5.710076\n","Train - Epoch 3, Batch: 12000, Loss: 5.988033\n","Train - Epoch 3, Batch: 13000, Loss: 6.583677\n","Train - Epoch 3, Batch: 14000, Loss: 6.556019\n","Train - Epoch 3, Batch: 15000, Loss: 7.435449\n","Train - Epoch 3, Batch: 16000, Loss: 7.457371\n","Train - Epoch 3, Batch: 17000, Loss: 5.998780\n","Train - Epoch 3, Batch: 18000, Loss: 5.824308\n","Train - Epoch 3, Batch: 19000, Loss: 6.419497\n","Train - Epoch 3, Batch: 20000, Loss: 6.292051\n","Train - Epoch 3, Batch: 21000, Loss: 6.293082\n","Train - Epoch 3, Batch: 22000, Loss: 5.791517\n","Train - Epoch 3, Batch: 23000, Loss: 6.441198\n","Train - Epoch 3, Batch: 24000, Loss: 6.878097\n","Train - Epoch 3, Batch: 25000, Loss: 6.790356\n","Train - Epoch 3, Batch: 26000, Loss: 6.613606\n","Train - Epoch 3, Batch: 27000, Loss: 6.017672\n","Train - Epoch 3, Batch: 28000, Loss: 6.092005\n","Train - Epoch 3, Batch: 29000, Loss: 4.930492\n"," Accuracy: 0.018117\n","Train - Epoch 4, Batch: 0, Loss: 4.068660\n","Train - Epoch 4, Batch: 1000, Loss: 5.926076\n","Train - Epoch 4, Batch: 2000, Loss: 4.973651\n","Train - Epoch 4, Batch: 3000, Loss: 5.333758\n","Train - Epoch 4, Batch: 4000, Loss: 5.285064\n","Train - Epoch 4, Batch: 5000, Loss: 5.939787\n","Train - Epoch 4, Batch: 6000, Loss: 5.511270\n","Train - Epoch 4, Batch: 7000, Loss: 5.910050\n","Train - Epoch 4, Batch: 8000, Loss: 6.014654\n","Train - Epoch 4, Batch: 9000, Loss: 5.794601\n","Train - Epoch 4, Batch: 10000, Loss: 5.020804\n","Train - Epoch 4, Batch: 11000, Loss: 4.938770\n","Train - Epoch 4, Batch: 12000, Loss: 5.108951\n","Train - Epoch 4, Batch: 13000, Loss: 5.694192\n","Train - Epoch 4, Batch: 14000, Loss: 5.721023\n","Train - Epoch 4, Batch: 15000, Loss: 6.506584\n","Train - Epoch 4, Batch: 16000, Loss: 6.511256\n","Train - Epoch 4, Batch: 17000, Loss: 5.196945\n","Train - Epoch 4, Batch: 18000, Loss: 4.978707\n","Train - Epoch 4, Batch: 19000, Loss: 5.575003\n","Train - Epoch 4, Batch: 20000, Loss: 5.456730\n","Train - Epoch 4, Batch: 21000, Loss: 5.491354\n","Train - Epoch 4, Batch: 22000, Loss: 5.014015\n","Train - Epoch 4, Batch: 23000, Loss: 5.514924\n","Train - Epoch 4, Batch: 24000, Loss: 6.121098\n","Train - Epoch 4, Batch: 25000, Loss: 5.969930\n","Train - Epoch 4, Batch: 26000, Loss: 5.882344\n","Train - Epoch 4, Batch: 27000, Loss: 5.266047\n","Train - Epoch 4, Batch: 28000, Loss: 5.372813\n","Train - Epoch 4, Batch: 29000, Loss: 4.292073\n"," Accuracy: 0.018233\n","Train - Epoch 5, Batch: 0, Loss: 3.583485\n","Train - Epoch 5, Batch: 1000, Loss: 5.220019\n","Train - Epoch 5, Batch: 2000, Loss: 4.337227\n","Train - Epoch 5, Batch: 3000, Loss: 4.711952\n","Train - Epoch 5, Batch: 4000, Loss: 4.668559\n","Train - Epoch 5, Batch: 5000, Loss: 5.268889\n","Train - Epoch 5, Batch: 6000, Loss: 4.845709\n","Train - Epoch 5, Batch: 7000, Loss: 5.184279\n","Train - Epoch 5, Batch: 8000, Loss: 5.318815\n","Train - Epoch 5, Batch: 9000, Loss: 5.127498\n","Train - Epoch 5, Batch: 10000, Loss: 4.397592\n","Train - Epoch 5, Batch: 11000, Loss: 4.411390\n","Train - Epoch 5, Batch: 12000, Loss: 4.493495\n","Train - Epoch 5, Batch: 13000, Loss: 5.066616\n","Train - Epoch 5, Batch: 14000, Loss: 5.100754\n","Train - Epoch 5, Batch: 15000, Loss: 5.842787\n","Train - Epoch 5, Batch: 16000, Loss: 5.825224\n","Train - Epoch 5, Batch: 17000, Loss: 4.620204\n","Train - Epoch 5, Batch: 18000, Loss: 4.383833\n","Train - Epoch 5, Batch: 19000, Loss: 4.958614\n","Train - Epoch 5, Batch: 20000, Loss: 4.821113\n","Train - Epoch 5, Batch: 21000, Loss: 4.918058\n","Train - Epoch 5, Batch: 22000, Loss: 4.445057\n","Train - Epoch 5, Batch: 23000, Loss: 4.861189\n","Train - Epoch 5, Batch: 24000, Loss: 5.519186\n","Train - Epoch 5, Batch: 25000, Loss: 5.358291\n","Train - Epoch 5, Batch: 26000, Loss: 5.317797\n","Train - Epoch 5, Batch: 27000, Loss: 4.705997\n","Train - Epoch 5, Batch: 28000, Loss: 4.835742\n","Train - Epoch 5, Batch: 29000, Loss: 3.814540\n"," Accuracy: 0.018550\n","Train - Epoch 6, Batch: 0, Loss: 3.213129\n","Train - Epoch 6, Batch: 1000, Loss: 4.667024\n","Train - Epoch 6, Batch: 2000, Loss: 3.854083\n","Train - Epoch 6, Batch: 3000, Loss: 4.262251\n","Train - Epoch 6, Batch: 4000, Loss: 4.190995\n","Train - Epoch 6, Batch: 5000, Loss: 4.762683\n","Train - Epoch 6, Batch: 6000, Loss: 4.343944\n","Train - Epoch 6, Batch: 7000, Loss: 4.648342\n","Train - Epoch 6, Batch: 8000, Loss: 4.797599\n","Train - Epoch 6, Batch: 9000, Loss: 4.648125\n","Train - Epoch 6, Batch: 10000, Loss: 3.912233\n","Train - Epoch 6, Batch: 11000, Loss: 4.008894\n","Train - Epoch 6, Batch: 12000, Loss: 4.044861\n","Train - Epoch 6, Batch: 13000, Loss: 4.580001\n","Train - Epoch 6, Batch: 14000, Loss: 4.623634\n","Train - Epoch 6, Batch: 15000, Loss: 5.314188\n","Train - Epoch 6, Batch: 16000, Loss: 5.283769\n","Train - Epoch 6, Batch: 17000, Loss: 4.172457\n","Train - Epoch 6, Batch: 18000, Loss: 3.928005\n","Train - Epoch 6, Batch: 19000, Loss: 4.461372\n","Train - Epoch 6, Batch: 20000, Loss: 4.331920\n","Train - Epoch 6, Batch: 21000, Loss: 4.454619\n","Train - Epoch 6, Batch: 22000, Loss: 4.016406\n","Train - Epoch 6, Batch: 23000, Loss: 4.365493\n","Train - Epoch 6, Batch: 24000, Loss: 5.042867\n","Train - Epoch 6, Batch: 25000, Loss: 4.881588\n","Train - Epoch 6, Batch: 26000, Loss: 4.862300\n","Train - Epoch 6, Batch: 27000, Loss: 4.254156\n","Train - Epoch 6, Batch: 28000, Loss: 4.396992\n","Train - Epoch 6, Batch: 29000, Loss: 3.439316\n"," Accuracy: 0.019017\n","Train - Epoch 7, Batch: 0, Loss: 2.929053\n","Train - Epoch 7, Batch: 1000, Loss: 4.231815\n","Train - Epoch 7, Batch: 2000, Loss: 3.472543\n","Train - Epoch 7, Batch: 3000, Loss: 3.887275\n","Train - Epoch 7, Batch: 4000, Loss: 3.822239\n","Train - Epoch 7, Batch: 5000, Loss: 4.366297\n","Train - Epoch 7, Batch: 6000, Loss: 3.963609\n","Train - Epoch 7, Batch: 7000, Loss: 4.230215\n","Train - Epoch 7, Batch: 8000, Loss: 4.397062\n","Train - Epoch 7, Batch: 9000, Loss: 4.265157\n","Train - Epoch 7, Batch: 10000, Loss: 3.530658\n","Train - Epoch 7, Batch: 11000, Loss: 3.677056\n","Train - Epoch 7, Batch: 12000, Loss: 3.685403\n","Train - Epoch 7, Batch: 13000, Loss: 4.182204\n","Train - Epoch 7, Batch: 14000, Loss: 4.228966\n","Train - Epoch 7, Batch: 15000, Loss: 4.865710\n","Train - Epoch 7, Batch: 16000, Loss: 4.849434\n","Train - Epoch 7, Batch: 17000, Loss: 3.803192\n","Train - Epoch 7, Batch: 18000, Loss: 3.582757\n","Train - Epoch 7, Batch: 19000, Loss: 4.048864\n","Train - Epoch 7, Batch: 20000, Loss: 3.940068\n","Train - Epoch 7, Batch: 21000, Loss: 4.078528\n","Train - Epoch 7, Batch: 22000, Loss: 3.657692\n","Train - Epoch 7, Batch: 23000, Loss: 3.959285\n","Train - Epoch 7, Batch: 24000, Loss: 4.659338\n","Train - Epoch 7, Batch: 25000, Loss: 4.494277\n","Train - Epoch 7, Batch: 26000, Loss: 4.491054\n","Train - Epoch 7, Batch: 27000, Loss: 3.885148\n","Train - Epoch 7, Batch: 28000, Loss: 4.046289\n","Train - Epoch 7, Batch: 29000, Loss: 3.150816\n"," Accuracy: 0.019317\n","Train - Epoch 8, Batch: 0, Loss: 2.699819\n","Train - Epoch 8, Batch: 1000, Loss: 3.889070\n","Train - Epoch 8, Batch: 2000, Loss: 3.174861\n","Train - Epoch 8, Batch: 3000, Loss: 3.586582\n","Train - Epoch 8, Batch: 4000, Loss: 3.535084\n","Train - Epoch 8, Batch: 5000, Loss: 4.035067\n","Train - Epoch 8, Batch: 6000, Loss: 3.654730\n","Train - Epoch 8, Batch: 7000, Loss: 3.887463\n","Train - Epoch 8, Batch: 8000, Loss: 4.060539\n","Train - Epoch 8, Batch: 9000, Loss: 3.942727\n","Train - Epoch 8, Batch: 10000, Loss: 3.234525\n","Train - Epoch 8, Batch: 11000, Loss: 3.407354\n","Train - Epoch 8, Batch: 12000, Loss: 3.400079\n","Train - Epoch 8, Batch: 13000, Loss: 3.853786\n","Train - Epoch 8, Batch: 14000, Loss: 3.910980\n","Train - Epoch 8, Batch: 15000, Loss: 4.485920\n","Train - Epoch 8, Batch: 16000, Loss: 4.510324\n","Train - Epoch 8, Batch: 17000, Loss: 3.499891\n","Train - Epoch 8, Batch: 18000, Loss: 3.309014\n","Train - Epoch 8, Batch: 19000, Loss: 3.714341\n","Train - Epoch 8, Batch: 20000, Loss: 3.620300\n","Train - Epoch 8, Batch: 21000, Loss: 3.768628\n","Train - Epoch 8, Batch: 22000, Loss: 3.359927\n","Train - Epoch 8, Batch: 23000, Loss: 3.620861\n","Train - Epoch 8, Batch: 24000, Loss: 4.340548\n","Train - Epoch 8, Batch: 25000, Loss: 4.162600\n","Train - Epoch 8, Batch: 26000, Loss: 4.184169\n","Train - Epoch 8, Batch: 27000, Loss: 3.584277\n","Train - Epoch 8, Batch: 28000, Loss: 3.746648\n","Train - Epoch 8, Batch: 29000, Loss: 2.918776\n"," Accuracy: 0.019450\n","Train - Epoch 9, Batch: 0, Loss: 2.506529\n","Train - Epoch 9, Batch: 1000, Loss: 3.595337\n","Train - Epoch 9, Batch: 2000, Loss: 2.931232\n","Train - Epoch 9, Batch: 3000, Loss: 3.326020\n","Train - Epoch 9, Batch: 4000, Loss: 3.300501\n","Train - Epoch 9, Batch: 5000, Loss: 3.761986\n","Train - Epoch 9, Batch: 6000, Loss: 3.389048\n","Train - Epoch 9, Batch: 7000, Loss: 3.585795\n","Train - Epoch 9, Batch: 8000, Loss: 3.774205\n","Train - Epoch 9, Batch: 9000, Loss: 3.675675\n","Train - Epoch 9, Batch: 10000, Loss: 2.993363\n","Train - Epoch 9, Batch: 11000, Loss: 3.175108\n","Train - Epoch 9, Batch: 12000, Loss: 3.161428\n","Train - Epoch 9, Batch: 13000, Loss: 3.573543\n","Train - Epoch 9, Batch: 14000, Loss: 3.645002\n","Train - Epoch 9, Batch: 15000, Loss: 4.176713\n","Train - Epoch 9, Batch: 16000, Loss: 4.218960\n","Train - Epoch 9, Batch: 17000, Loss: 3.254906\n","Train - Epoch 9, Batch: 18000, Loss: 3.084178\n","Train - Epoch 9, Batch: 19000, Loss: 3.444294\n","Train - Epoch 9, Batch: 20000, Loss: 3.339994\n","Train - Epoch 9, Batch: 21000, Loss: 3.502284\n","Train - Epoch 9, Batch: 22000, Loss: 3.115654\n","Train - Epoch 9, Batch: 23000, Loss: 3.336084\n","Train - Epoch 9, Batch: 24000, Loss: 4.058399\n","Train - Epoch 9, Batch: 25000, Loss: 3.882176\n","Train - Epoch 9, Batch: 26000, Loss: 3.910080\n","Train - Epoch 9, Batch: 27000, Loss: 3.330196\n","Train - Epoch 9, Batch: 28000, Loss: 3.499071\n","Train - Epoch 9, Batch: 29000, Loss: 2.719825\n"," Accuracy: 0.019667\n"],"name":"stdout"}]}]}